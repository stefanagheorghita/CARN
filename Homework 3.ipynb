{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30787,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stefanagheorghita/CARN/blob/main/Homework%203.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "sweep_config = {\n",
        "    \"method\": \"grid\",\n",
        "    \"metric\": {\n",
        "        \"name\": \"val_accuracy\",\n",
        "        \"goal\": \"maximize\"\n",
        "    },\n",
        "    \"parameters\": {\n",
        "        \"dataset\": {\"value\": \"CIFAR100\"},\n",
        "        \"data_path\": {\"value\": \"./data\"},\n",
        "        \"model_name\": {\"values\": [ \"resnet18_resize\"]},\n",
        "        \"num_classes\": {\"value\": 100},\n",
        "        \"batch_size\": {\"values\": [64]},\n",
        "        \"num_epochs\": {\"values\": [100]},\n",
        "         # \"learning_rate\": {\n",
        "        \"optimizer_config\": {\n",
        "        \"values\": [\n",
        "            {\"optimizer\": \"adamw\", \"learning_rate\": 0.0005},\n",
        "             {\"optimizer\": \"adamw\", \"learning_rate\": 0.001},\n",
        "             {\"optimizer\": \"sgd\", \"learning_rate\": 0.01}\n",
        "        ]\n",
        "    },\n",
        "        \"weight_decay\": {\"value\": 0.0005},\n",
        "        # \"optimizer\": {\"values\": [\"adamw\", \"sgd\"]},\n",
        "        \"momentum\": {\"value\": 0.9},\n",
        "        \"nesterov\": {\"value\": True},\n",
        "        \"patience\": {\"value\": 3},\n",
        "        \"stop_mode\": {\"value\": \"max\"},\n",
        "        \"min_delta\": {\"value\": 0.0001},\n",
        "        \"scheduler\": {\"values\": [\"cosineannealinglr\"]},\n",
        "        \"t_max\": {\"values\": [100]},\n",
        "        \"eta_min\": {\"value\": 0.00001},\n",
        "        \"augmentation_scheme\": {\"values\": [\"randaugment\", \"combined\"]},\n",
        "        \"use_cutmix\": {\"values\": [True]},\n",
        "        \"use_mixup\": {\"values\": [True]},\n",
        "        \"alpha\": {\"value\": 1.0},\n",
        "        \"t_0\": {\"value\": 10},\n",
        "        \"t_mult\": {\"value\": 2},\n",
        "        \"warmup\": {\"value\": 5},\n",
        "        \"patience_early_stopping\": {\"value\": 10},\n",
        "        \"pretrained\": {\"value\": True}\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "with open(\"sweep_config.json\", \"w\") as f:\n",
        "    json.dump(sweep_config, f)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-04T10:11:49.77149Z",
          "iopub.execute_input": "2024-11-04T10:11:49.772198Z",
          "iopub.status.idle": "2024-11-04T10:11:49.795537Z",
          "shell.execute_reply.started": "2024-11-04T10:11:49.772151Z",
          "shell.execute_reply": "2024-11-04T10:11:49.794081Z"
        },
        "id": "9fxxv680ANmH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import json\n",
        "\n",
        "import yaml\n",
        "from torch import nn, Tensor\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "import torch.backends.cudnn\n",
        "\n",
        "benchmark = True\n",
        "class PreActBlock(nn.Module):\n",
        "    \"\"\"Pre-activation version of the BasicBlock.\"\"\"\n",
        "\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(PreActBlock, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False\n",
        "        )\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            planes, planes, kernel_size=3, stride=1, padding=1, bias=False\n",
        "        )\n",
        "\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Conv2d(\n",
        "                in_planes,\n",
        "                self.expansion * planes,\n",
        "                kernel_size=1,\n",
        "                stride=stride,\n",
        "                bias=False,\n",
        "            )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        out = F.relu(self.bn1(x), inplace=True)\n",
        "        shortcut = self.shortcut(out) if hasattr(self, \"shortcut\") else x\n",
        "        out = self.conv1(out)\n",
        "        out = self.conv2(F.relu(self.bn2(out), inplace=True))\n",
        "        out += shortcut\n",
        "        return out\n",
        "\n",
        "\n",
        "class PreActBottleneck(nn.Module):\n",
        "    \"\"\"Pre-activation version of the original Bottleneck module.\"\"\"\n",
        "\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(PreActBottleneck, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            planes, planes, kernel_size=3, stride=stride, padding=1, bias=False\n",
        "        )\n",
        "        self.bn3 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(\n",
        "            planes, self.expansion * planes, kernel_size=1, bias=False\n",
        "        )\n",
        "\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Conv2d(\n",
        "                in_planes,\n",
        "                self.expansion * planes,\n",
        "                kernel_size=1,\n",
        "                stride=stride,\n",
        "                bias=False,\n",
        "            )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        out = F.relu(self.bn1(x), inplace=True)\n",
        "        shortcut = self.shortcut(out) if hasattr(self, \"shortcut\") else x\n",
        "        out = self.conv1(out)\n",
        "        out = self.conv2(F.relu(self.bn2(out), inplace=True))\n",
        "        out = self.conv3(F.relu(self.bn3(out), inplace=True))\n",
        "        out += shortcut\n",
        "        return out\n",
        "\n",
        "\n",
        "class PreActResNet_C10(nn.Module):\n",
        "    \"\"\"Pre-activation ResNet for CIFAR-10\"\"\"\n",
        "\n",
        "    def __init__(self, block, num_blocks, num_classes):\n",
        "        super(PreActResNet_C10, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        out = self.conv1(x)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def PreActResNet18_C10(num_classes):\n",
        "    return PreActResNet_C10(PreActBlock, [2, 2, 2, 2], num_classes)\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            torch.nn.Flatten(),\n",
        "            torch.nn.Linear(784, 256),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "class CustomMLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_layers, output_size, activation_fn=nn.ReLU):\n",
        "        super(CustomMLP, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        current_input_size = input_size\n",
        "\n",
        "        for hidden_size in hidden_layers:\n",
        "            layers.append(nn.Linear(current_input_size, hidden_size))\n",
        "            layers.append(activation_fn())\n",
        "            current_input_size = hidden_size\n",
        "\n",
        "        layers.append(nn.Linear(current_input_size, output_size))\n",
        "\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "\n",
        "model = CustomMLP(input_size=784, hidden_layers=[256, 128], output_size=10, activation_fn=nn.Sigmoid)\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self, num_classes=10, in_channels=1):\n",
        "        super(LeNet, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, 6, kernel_size=5, stride=1, padding=2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "from typing import Literal, cast\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torchvision.datasets import MNIST, CIFAR10, CIFAR100\n",
        "from torchvision.transforms.v2 import CutMix, MixUp\n",
        "\n",
        "from torchvision.transforms import v2\n",
        "from timm import create_model\n",
        "import os\n",
        "import pickle\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import AutoAugment, AutoAugmentPolicy\n",
        "\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_device():\n",
        "    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "def cache_dataset(dataset_class, data_dir, cache_dir='./cache', train=True):\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    subset = 'train' if train else 'test'\n",
        "    cache_path = os.path.join(cache_dir, f'{dataset_class.__name__}_{subset}.pkl')\n",
        "\n",
        "    if os.path.exists(cache_path):\n",
        "        with open(cache_path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "    else:\n",
        "        data = dataset_class(root=data_dir, train=train, download=True)\n",
        "        with open(cache_path, 'wb') as f:\n",
        "            pickle.dump(data, f)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def get_data_augmentation(scheme=\"basic\", dataset=\"CIFAR\"):\n",
        "    if dataset == \"MNIST\":\n",
        "        if scheme == \"basic\":\n",
        "            train_transform = v2.Compose(\n",
        "                [v2.ToImage(), v2.ToDtype(torch.float32, scale=True), v2.Normalize((0.5,), (0.5,))])\n",
        "        elif scheme == \"random_flip\":\n",
        "            train_transform = v2.Compose(\n",
        "                [v2.RandomHorizontalFlip(), v2.ToImage(), v2.ToDtype(torch.float32, scale=True),\n",
        "                 v2.Normalize((0.5,), (0.5,))])\n",
        "        elif scheme == \"random_crop_flip\":\n",
        "            train_transform = v2.Compose(\n",
        "                [v2.RandomResizedCrop(28, scale=(0.8, 1.0)), v2.RandomHorizontalFlip(), v2.ToImage(),\n",
        "                 v2.ToDtype(torch.float32, scale=True),\n",
        "                 v2.Normalize((0.5,), (0.5,))])\n",
        "        elif scheme == \"randaugment\":\n",
        "            train_transform = v2.Compose(\n",
        "                [v2.RandAugment(), v2.ToImage(), v2.ToDtype(torch.float32, scale=True), v2.Normalize((0.5,), (0.5,))])\n",
        "        elif scheme == \"combined\":\n",
        "            train_transform = v2.Compose(\n",
        "                [ v2.RandomResizedCrop(28, scale=(0.8, 1.0)), v2.RandomRotation(15), v2.RandomHorizontalFlip(),\n",
        "                 v2.RandAugment(), v2.ToImage(), v2.ToDtype(torch.float32, scale=True), v2.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Augmentation scheme '{scheme}' not supported for MNIST.\")\n",
        "        test_transform = v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True), v2.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "    elif dataset == \"CIFAR\":\n",
        "        if scheme == \"basic\":\n",
        "            train_transform = v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True),\n",
        "                                          v2.Normalize((0.5, 0.5, 0.5), (0.25, 0.25, 0.25))])\n",
        "        elif scheme == \"random_flip\":\n",
        "            train_transform = v2.Compose(\n",
        "                [v2.RandomHorizontalFlip(), v2.ToImage(), v2.ToDtype(torch.float32, scale=True),\n",
        "                 v2.Normalize((0.5, 0.5, 0.5), (0.25, 0.25, 0.25))])\n",
        "        elif scheme == \"random_crop_flip\":\n",
        "            train_transform = v2.Compose([v2.RandomCrop(32, padding=4), v2.RandomHorizontalFlip(), v2.ToImage(),\n",
        "                                          v2.ToDtype(torch.float32, scale=True),\n",
        "                                          v2.Normalize((0.5, 0.5, 0.5), (0.25, 0.25, 0.25))])\n",
        "        elif scheme == \"randaugment\":\n",
        "            train_transform = v2.Compose(\n",
        "                [v2.RandAugment(), v2.ToImage(), v2.ToDtype(torch.float32, scale=True),\n",
        "                 v2.Normalize((0.5, 0.5, 0.5), (0.25, 0.25, 0.25))])\n",
        "        elif scheme == \"autoaugment\":\n",
        "            train_transform = v2.Compose(\n",
        "                [AutoAugment(policy=AutoAugmentPolicy.CIFAR10), v2.ToImage(), v2.ToDtype(torch.float32, scale=True),\n",
        "                 v2.Normalize((0.5, 0.5, 0.5), (0.25, 0.25, 0.25))])\n",
        "\n",
        "        elif scheme == \"combined\":\n",
        "            train_transform = v2.Compose(\n",
        "                [v2.RandomResizedCrop(32, scale=(0.8, 1.0)), v2.RandomHorizontalFlip(), v2.RandomRotation(15),\n",
        "                 v2.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1), v2.RandAugment(), v2.ToImage(), v2.ToDtype(torch.float32, scale=True),\n",
        "                 v2.Normalize((0.5, 0.5, 0.5), (0.25, 0.25, 0.25))])\n",
        "        elif scheme == \"combined2\":\n",
        "            train_transform = v2.Compose(\n",
        "                [AutoAugment(policy=AutoAugmentPolicy.CIFAR10), v2.RandomCrop(32, padding=4), v2.RandomHorizontalFlip(),\n",
        "                 v2.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "                 v2.RandomRotation(15), v2.AutoAugment(), v2.ToImage(), v2.ToDtype(torch.float32, scale=True),\n",
        "                 v2.Normalize((0.5,), (0.5,))])\n",
        "        elif scheme == \"combined_resize\":\n",
        "                train_transform = v2.Compose([\n",
        "                    v2.Resize((64, 64)),\n",
        "                    v2.RandomResizedCrop(64, scale=(0.8, 1.0)),\n",
        "                    v2.RandomRotation(15),\n",
        "                    v2.RandomHorizontalFlip(),\n",
        "                    v2.RandAugment(),\n",
        "                    v2.ToImage(),\n",
        "                    v2.ToDtype(torch.float32, scale=True),\n",
        "                    v2.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "                ])\n",
        "        elif scheme == \"combined_resize2\":\n",
        "                    train_transform = v2.Compose([\n",
        "                        v2.RandomRotation(10),\n",
        "                        v2.RandomResizedCrop(32, scale=(0.9, 1.1)),\n",
        "                        v2.RandomHorizontalFlip(),\n",
        "                        v2.RandomAffine(degrees=0, shear=10),\n",
        "                        v2.RandomCrop(32, padding=3),\n",
        "                        v2.ToImage(),\n",
        "                        v2.ToDtype(torch.float32, scale=True),\n",
        "                        v2.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "                    ])\n",
        "        else:\n",
        "            raise ValueError(f\"Augmentation scheme '{scheme}' not supported for CIFAR.\")\n",
        "        test_transform = v2.Compose(\n",
        "            [v2.ToImage(), v2.ToDtype(torch.float32, scale=True), v2.Normalize((0.5, 0.5, 0.5), (0.25, 0.25, 0.25))])\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Dataset '{dataset}' not supported.\")\n",
        "\n",
        "    return train_transform, test_transform\n",
        "\n",
        "\n",
        "class CutMixMixUp:\n",
        "    def __init__(self, num_classes, alpha_cutmix=1.0, alpha_mixup=1.0):\n",
        "        self.cutmix = CutMix(num_classes=num_classes, alpha=alpha_cutmix)\n",
        "        self.mixup = MixUp(num_classes=num_classes, alpha=alpha_mixup)\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        batch = self.cutmix(batch)\n",
        "        batch = self.mixup(batch)\n",
        "        return batch\n",
        "\n",
        "\n",
        "def load_data(dataset_class, data_dir, batch_size=64, cache_dir='./cache', scheme=\"basic\", custom_transforms=None,\n",
        "              shuffle=True, use_cutmix=False, use_mixup=False, alpha=1.0):\n",
        "    if custom_transforms:\n",
        "        train_transform, test_transform = custom_transforms\n",
        "    else:\n",
        "        if dataset_class in [MNIST]:\n",
        "            train_transform, test_transform = get_data_augmentation(scheme=scheme, dataset=\"MNIST\")\n",
        "        elif dataset_class in [CIFAR10, CIFAR100]:\n",
        "            train_transform, test_transform = get_data_augmentation(scheme=scheme, dataset=\"CIFAR\")\n",
        "        else:\n",
        "            raise ValueError(\"Unknown dataset, please specify valid transforms.\")\n",
        "\n",
        "    try:\n",
        "        train_data = cache_dataset(dataset_class, data_dir, cache_dir, train=True)\n",
        "        test_data = cache_dataset(dataset_class, data_dir, cache_dir, train=False)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset: {e}\")\n",
        "        raise\n",
        "\n",
        "    train_data.transform = train_transform\n",
        "    test_data.transform = test_transform\n",
        "\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=shuffle, num_workers=4)\n",
        "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "def get_model(dataset, model_name, num_classes, input_size=None, hidden_layers=None, pretrained = True):\n",
        "    if dataset == 'CIFAR10' or dataset == 'CIFAR100':\n",
        "        if model_name in ['resnet18', 'resnet18_resize'] :\n",
        "            model = create_model(\"resnet18\", pretrained=pretrained, num_classes=num_classes)\n",
        "        elif model_name in ['resnet50', 'resnet50_resize']:\n",
        "            model = create_model(\"resnet50\", pretrained=pretrained, num_classes=num_classes)\n",
        "        elif model_name == 'resnet18_cifar10':\n",
        "            model = create_model(\"hf_hub:edadaltocg/resnet18_cifar10\", pretrained=False, num_classes=num_classes)\n",
        "            model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        elif model_name == 'preactresnet18':\n",
        "            model = PreActResNet18_C10(num_classes=num_classes)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Model '{model_name}' is not supported for CIFAR.\")\n",
        "        if pretrained and model_name in ['resnet18_resize', 'resnet50_resize']:\n",
        "            model = nn.Sequential(\n",
        "                nn.Upsample(size=(224, 224), mode='bilinear', align_corners=False),\n",
        "                model\n",
        "            )\n",
        "\n",
        "\n",
        "    elif dataset == 'MNIST':\n",
        "        if model_name.upper() == \"MLP\":\n",
        "            model = MLP(num_classes=num_classes)\n",
        "        elif model_name.upper() == \"LENET\":\n",
        "            model = LeNet(num_classes=num_classes, in_channels=1)\n",
        "        elif model_name.upper() == \"CUSTOMMLP\":\n",
        "            model = CustomMLP(input_size=input_size, hidden_layers=hidden_layers, output_size=num_classes)\n",
        "        else:\n",
        "            raise ValueError(f\"Model '{model_name}' is not supported for MNIST. Choose 'MLP' or 'LeNet'.\")\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Dataset '{dataset}' is not supported. Choose 'CIFAR10', 'CIFAR100', or 'MNIST'.\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "def get_optimizer(optimizer_name, model_parameters, lr=0.001, momentum=0.9, weight_decay=0.0, nesterov=True):\n",
        "    optimizer_name = optimizer_name.lower()\n",
        "\n",
        "    if optimizer_name == 'sgd':\n",
        "        return optim.SGD(model_parameters, lr=lr)\n",
        "    elif optimizer_name == 'sgd_momentum':\n",
        "        return optim.SGD(model_parameters, lr=lr, momentum=momentum)\n",
        "    elif optimizer_name == 'sgd_nesterov':\n",
        "        return optim.SGD(model_parameters, lr=lr, momentum=momentum, nesterov=nesterov)\n",
        "    elif optimizer_name == 'sgd_weight_decay':\n",
        "        return optim.SGD(model_parameters, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "    elif optimizer_name == 'adam':\n",
        "        return optim.Adam(model_parameters, lr=lr, weight_decay=weight_decay)\n",
        "    elif optimizer_name == 'adamw':\n",
        "        return optim.AdamW(model_parameters, lr=lr, weight_decay=weight_decay)\n",
        "    elif optimizer_name == 'rmsprop':\n",
        "        return optim.RMSprop(model_parameters, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "    else:\n",
        "        raise ValueError(f\"Optimizer '{optimizer_name}' not supported.\")\n",
        "\n",
        "\n",
        "def get_scheduler(optimizer, scheduler_name, **kwargs):\n",
        "    scheduler_name = scheduler_name.lower()\n",
        "\n",
        "    if scheduler_name == 'steplr':\n",
        "        return lr_scheduler.StepLR(optimizer, step_size=kwargs.get('step_size', 10), gamma=kwargs.get('gamma', 0.1))\n",
        "\n",
        "    elif scheduler_name == 'reducelronplateau':\n",
        "        mode_str = kwargs.get('mode', 'min')\n",
        "        if mode_str not in ['min', 'max']:\n",
        "            raise ValueError(\"Invalid mode for ReduceLROnPlateau: must be 'min' or 'max'\")\n",
        "        mode = cast(Literal[\"min\", \"max\"], mode_str)\n",
        "        return lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer,\n",
        "            mode=mode,\n",
        "            factor=kwargs.get('factor', 0.1),\n",
        "            patience=kwargs.get('patience', 10)\n",
        "        )\n",
        "\n",
        "    elif scheduler_name == 'cosineannealinglr':\n",
        "        return lr_scheduler.CosineAnnealingLR(\n",
        "            optimizer,\n",
        "            T_max=kwargs.get('t_max', 50),\n",
        "            eta_min=kwargs.get('eta_min', 0)\n",
        "        )\n",
        "\n",
        "    elif scheduler_name == 'cosineannealingwarmrestarts':\n",
        "        return lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "            optimizer,\n",
        "            T_0=kwargs.get('t_0', 10),\n",
        "            T_mult=kwargs.get('t_mult', 1),\n",
        "            eta_min=kwargs.get('eta_min', 0)\n",
        "        )\n",
        "\n",
        "    elif scheduler_name == 'exponentiallr':\n",
        "        return lr_scheduler.ExponentialLR(optimizer, gamma=kwargs.get('gamma', 0.9))\n",
        "\n",
        "    elif scheduler_name == 'linearlr':\n",
        "        return lr_scheduler.LinearLR(\n",
        "            optimizer,\n",
        "            start_factor=kwargs.get('start_factor', 1.0),\n",
        "            end_factor=kwargs.get('end_factor', 0.0),\n",
        "            total_iters=kwargs.get('total_iters', 100)\n",
        "        )\n",
        "\n",
        "    elif scheduler_name == 'none':\n",
        "        return None\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Scheduler '{scheduler_name}' not supported.\")\n",
        "\n",
        "\n",
        "def early_stopping(current_score, best_score, patience_counter, patience_early_stopping, min_delta=0.0, mode=\"min\"):\n",
        "    if best_score is None:\n",
        "        best_score = current_score\n",
        "        return False, best_score, patience_counter\n",
        "\n",
        "    if mode == \"min\":\n",
        "        improvement = best_score - current_score > min_delta\n",
        "    elif mode == \"max\":\n",
        "        improvement = current_score - best_score > min_delta\n",
        "    else:\n",
        "        raise ValueError(\"Mode should be 'min' or 'max'\")\n",
        "\n",
        "    if improvement:\n",
        "        best_score = current_score\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    early_stop = patience_counter >= patience_early_stopping\n",
        "    return early_stop, best_score, patience_counter\n",
        "\n",
        "\n",
        "def validate_model(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss = running_loss / len(test_loader.dataset)\n",
        "    val_accuracy = 100 * correct / total\n",
        "    return val_loss, val_accuracy\n",
        "\n",
        "\n",
        "from torch.amp import autocast, GradScaler\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, test_loader, device, num_epochs, optimizer, num_classes, scheduler_mode=None,\n",
        "                scheduler=None, patience_early_stopping=5, min_delta=0.0, early_stop_mode=\"min\", learning_rate=0.1,\n",
        "                warmup=0, grad_alpha=1.0, use_cutmix=True, use_mixup=True, alpha=1.0):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scaler = GradScaler(device)\n",
        "\n",
        "    best_val_score = None\n",
        "    patience_counter = 0\n",
        "    best_val_accuracy = 0.0\n",
        "    wandb.watch(model, log=\"all\", log_freq=10)\n",
        "    alpha = float(alpha)\n",
        "    cutmix = v2.CutMix(num_classes=num_classes, alpha=alpha)\n",
        "    mixup = v2.MixUp(num_classes=num_classes, alpha=alpha)\n",
        "    cutmix_or_mixup = v2.RandomChoice([cutmix, mixup])\n",
        "    rand= random.randint(1000, 9999)\n",
        "    file_path = f\"/kaggle/working/best_model_{rand}.pth\"\n",
        "    for epoch in range(num_epochs):\n",
        "        print(\"Epoch \", epoch)\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct, total = 0, 0\n",
        "\n",
        "        if epoch < warmup:\n",
        "            lr_scale = min(1., float(epoch + 1) / warmup)\n",
        "            for pg in optimizer.param_groups:\n",
        "                pg['lr'] = learning_rate * lr_scale\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "            if use_cutmix and use_mixup:\n",
        "                inputs, labels = cutmix_or_mixup(inputs, labels)\n",
        "            elif use_cutmix:\n",
        "                inputs, labels = cutmix(inputs, labels)\n",
        "            elif use_mixup:\n",
        "                inputs, labels = mixup(inputs, labels)\n",
        "\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            with autocast(device):\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            #             scaler.unscale_(optimizer)\n",
        "            #             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_alpha)\n",
        "\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            if use_cutmix or use_mixup:\n",
        "                correct += predicted.eq(labels.argmax(dim=1)).sum().item()\n",
        "            else:\n",
        "                  correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = 100 * correct / total\n",
        "\n",
        "        val_loss, val_accuracy = validate_model(model, test_loader, criterion, device)\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            torch.save(model.state_dict(), file_path)\n",
        "            print(f\"Best model saved with accuracy: {best_val_accuracy:.2f}%\")\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} - \")\n",
        "        print(f\"Train Loss: {train_loss:.4f} - Train Accuracy: {train_accuracy:.2f}% - \")\n",
        "        print(f\"Val Loss: {val_loss:.4f} - Val Accuracy: {val_accuracy:.2f}%\")\n",
        "        wandb.log({\n",
        "            'epoch': epoch + 1,\n",
        "            'train_loss': train_loss,\n",
        "            'train_accuracy': train_accuracy,\n",
        "            'val_loss': val_loss,\n",
        "            'val_accuracy': val_accuracy\n",
        "        })\n",
        "\n",
        "        val_score = val_loss if early_stop_mode == \"min\" else val_accuracy\n",
        "\n",
        "        if isinstance(scheduler, lr_scheduler.ReduceLROnPlateau):\n",
        "            if scheduler_mode == \"max\":\n",
        "                scheduler.step(val_accuracy)\n",
        "            elif scheduler_mode == \"min\":\n",
        "                scheduler.step(val_loss)\n",
        "        elif scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "        early_stop, best_val_score, patience_counter = early_stopping(\n",
        "            current_score=val_score,\n",
        "            best_score=best_val_score,\n",
        "            patience_counter=patience_counter,\n",
        "            patience_early_stopping=patience_early_stopping,\n",
        "            min_delta=min_delta,\n",
        "            mode=early_stop_mode\n",
        "        )\n",
        "\n",
        "        if early_stop:\n",
        "            print(\"Early stopping triggered. Stopping training.\")\n",
        "            break\n",
        "\n",
        "    print(\"Training complete.\")\n",
        "\n",
        "\n",
        "import wandb\n",
        "\n",
        "\n",
        "def sweep_train():\n",
        "    wandb.init()\n",
        "    config = wandb.config\n",
        "    try:\n",
        "        dataset_class = datasets.CIFAR100 if config.dataset == \"CIFAR100\" else datasets.CIFAR10 if config.dataset == \"CIFAR10\" else datasets.MNIST\n",
        "        train_loader, test_loader = load_data(\n",
        "            dataset_class=dataset_class,\n",
        "            data_dir=config.data_path,\n",
        "            batch_size=config.batch_size,\n",
        "            scheme=config.augmentation_scheme,\n",
        "            use_cutmix=config.use_cutmix,\n",
        "            use_mixup=config.use_mixup,\n",
        "            alpha=config.alpha\n",
        "        )\n",
        "\n",
        "        model = get_model(\n",
        "            dataset=config.dataset,\n",
        "            model_name=config.model_name,\n",
        "            num_classes=config.num_classes\n",
        "        )\n",
        "        optimizer_name = config.optimizer_config[\"optimizer\"]\n",
        "        learning_rate = config.optimizer_config[\"learning_rate\"]\n",
        "\n",
        "        optimizer = get_optimizer(\n",
        "            optimizer_name=optimizer_name,\n",
        "            model_parameters=model.parameters(),\n",
        "            lr=learning_rate,\n",
        "            momentum=config.momentum,\n",
        "            weight_decay=config.weight_decay,\n",
        "            nesterov=config.nesterov\n",
        "        )\n",
        "\n",
        "        scheduler = get_scheduler(\n",
        "            optimizer=optimizer,\n",
        "            scheduler_name=config.scheduler,\n",
        "            t_max=config.get('t_max', 200),\n",
        "            eta_min=config.get('eta_min', 0),\n",
        "            step_size=config.get('step_size', 10),\n",
        "            gamma=config.get('gamma', 0.1),\n",
        "            patience=config.get('scheduler_patience', 10),\n",
        "            factor=config.get('factor', 0.1)\n",
        "        )\n",
        "\n",
        "        train_model(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            test_loader=test_loader,\n",
        "            device=get_device(),\n",
        "            num_epochs=config.num_epochs,\n",
        "            optimizer=optimizer,\n",
        "            scheduler=scheduler,\n",
        "            patience_early_stopping=config.patience_early_stopping,\n",
        "            min_delta=config.min_delta,\n",
        "            early_stop_mode=config.stop_mode,\n",
        "            learning_rate=learning_rate,\n",
        "            num_classes=config.num_classes,\n",
        "            use_cutmix=config.use_cutmix,\n",
        "            use_mixup=config.use_mixup,\n",
        "            alpha=config.alpha,\n",
        "            warmup=config.warmup,\n",
        "        )\n",
        "    finally:\n",
        "        wandb.finish()\n",
        "\n",
        "def load_config(file_path):\n",
        "    ext = os.path.splitext(file_path)[-1].lower()\n",
        "    if ext == \".json\":\n",
        "        with open(file_path, 'r') as f:\n",
        "            config = json.load(f)\n",
        "    elif ext in {\".yaml\", \".yml\"}:\n",
        "        with open(file_path, 'r') as f:\n",
        "            config = yaml.safe_load(f)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Use JSON or YAML.\")\n",
        "    return config\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "config_file_path = \"sweep_config.json\"\n",
        "sweep_config = load_config(config_file_path)\n",
        "\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"training-cifar100\")\n",
        "wandb.agent(sweep_id, sweep_train)"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-04T10:11:49.902025Z",
          "iopub.execute_input": "2024-11-04T10:11:49.902408Z",
          "iopub.status.idle": "2024-11-04T11:20:31.898254Z",
          "shell.execute_reply.started": "2024-11-04T10:11:49.90237Z",
          "shell.execute_reply": "2024-11-04T11:20:31.897122Z"
        },
        "id": "CATdyo0eANmI",
        "outputId": "bc895052-a089-466c-d7d4-79440a099dd4",
        "colab": {
          "referenced_widgets": [
            "f97cff47ad964d25aa313479d10edfc8",
            "797ebeec3a8e4a83975f305b321731ab",
            "29f7b6735e504af88b58e0d31b54a2b7",
            "1e98196c44904740afb16833110b0e33",
            "36e35d4ba8f447f891937723e9aa2613"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "  ········\n"
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Create sweep with ID: z9ns4r8w\nSweep URL: https://wandb.ai/gheorghitastefana-alexandru-ioan-cuza-university-iasi/training-cifar100/sweeps/z9ns4r8w\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cg9to7ic with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \talpha: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \taugmentation_scheme: randaugment\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: ./data\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdataset: CIFAR100\n\u001b[34m\u001b[1mwandb\u001b[0m: \teta_min: 1e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_delta: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_name: resnet18_resize\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.9\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnesterov: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_classes: 100\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 20\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_config: {'learning_rate': 0.0005, 'optimizer': 'adamw'}\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpatience: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpatience_early_stopping: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpretrained: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler: cosineannealinglr\n\u001b[34m\u001b[1mwandb\u001b[0m: \tstop_mode: max\n\u001b[34m\u001b[1mwandb\u001b[0m: \tt_0: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tt_max: 100\n\u001b[34m\u001b[1mwandb\u001b[0m: \tt_mult: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_cutmix: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_mixup: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0005\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgheorghitastefana\u001b[0m (\u001b[33mgheorghitastefana-alexandru-ioan-cuza-university-iasi\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113503688888058, max=1.0…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f97cff47ad964d25aa313479d10edfc8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.18.3"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20241104_101214-cg9to7ic</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/gheorghitastefana-alexandru-ioan-cuza-university-iasi/training-cifar100/runs/cg9to7ic' target=\"_blank\">generous-sweep-1</a></strong> to <a href='https://wandb.ai/gheorghitastefana-alexandru-ioan-cuza-university-iasi/training-cifar100' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/gheorghitastefana-alexandru-ioan-cuza-university-iasi/training-cifar100/sweeps/z9ns4r8w' target=\"_blank\">https://wandb.ai/gheorghitastefana-alexandru-ioan-cuza-university-iasi/training-cifar100/sweeps/z9ns4r8w</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/gheorghitastefana-alexandru-ioan-cuza-university-iasi/training-cifar100' target=\"_blank\">https://wandb.ai/gheorghitastefana-alexandru-ioan-cuza-university-iasi/training-cifar100</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View sweep at <a href='https://wandb.ai/gheorghitastefana-alexandru-ioan-cuza-university-iasi/training-cifar100/sweeps/z9ns4r8w' target=\"_blank\">https://wandb.ai/gheorghitastefana-alexandru-ioan-cuza-university-iasi/training-cifar100/sweeps/z9ns4r8w</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/gheorghitastefana-alexandru-ioan-cuza-university-iasi/training-cifar100/runs/cg9to7ic' target=\"_blank\">https://wandb.ai/gheorghitastefana-alexandru-ioan-cuza-university-iasi/training-cifar100/runs/cg9to7ic</a>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 169001437/169001437 [00:05<00:00, 29583936.81it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Extracting ./data/cifar-100-python.tar.gz to ./data\nFiles already downloaded and verified\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/46.8M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "797ebeec3a8e4a83975f305b321731ab"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Epoch  0\nBest model saved with accuracy: 33.45%\nEpoch 1/20 - \nTrain Loss: 4.2947 - Train Accuracy: 9.22% - \nVal Loss: 2.9145 - Val Accuracy: 33.45%\nEpoch  1\nBest model saved with accuracy: 62.73%\nEpoch 2/20 - \nTrain Loss: 3.2017 - Train Accuracy: 34.38% - \nVal Loss: 1.4617 - Val Accuracy: 62.73%\nEpoch  2\nBest model saved with accuracy: 69.37%\nEpoch 3/20 - \nTrain Loss: 2.7711 - Train Accuracy: 45.31% - \nVal Loss: 1.2885 - Val Accuracy: 69.37%\nEpoch  3\nBest model saved with accuracy: 73.10%\nEpoch 4/20 - \nTrain Loss: 2.5074 - Train Accuracy: 52.06% - \nVal Loss: 1.0508 - Val Accuracy: 73.10%\nEpoch  4\nBest model saved with accuracy: 75.66%\nEpoch 5/20 - \nTrain Loss: 2.4448 - Train Accuracy: 54.29% - \nVal Loss: 1.0654 - Val Accuracy: 75.66%\nEpoch  5\nBest model saved with accuracy: 76.42%\nEpoch 6/20 - \nTrain Loss: 2.3356 - Train Accuracy: 56.78% - \nVal Loss: 1.0818 - Val Accuracy: 76.42%\nEpoch  6\nBest model saved with accuracy: 77.44%\nEpoch 7/20 - \nTrain Loss: 2.2430 - Train Accuracy: 58.65% - \nVal Loss: 0.9964 - Val Accuracy: 77.44%\nEpoch  7\nBest model saved with accuracy: 78.11%\nEpoch 8/20 - \nTrain Loss: 2.2411 - Train Accuracy: 59.14% - \nVal Loss: 0.9324 - Val Accuracy: 78.11%\nEpoch  8\nBest model saved with accuracy: 78.31%\nEpoch 9/20 - \nTrain Loss: 2.1818 - Train Accuracy: 61.19% - \nVal Loss: 0.9169 - Val Accuracy: 78.31%\nEpoch  9\nBest model saved with accuracy: 79.12%\nEpoch 10/20 - \nTrain Loss: 2.1606 - Train Accuracy: 61.20% - \nVal Loss: 0.9102 - Val Accuracy: 79.12%\nEpoch  10\nBest model saved with accuracy: 79.60%\nEpoch 11/20 - \nTrain Loss: 2.1257 - Train Accuracy: 63.09% - \nVal Loss: 0.7880 - Val Accuracy: 79.60%\nEpoch  11\nBest model saved with accuracy: 79.94%\nEpoch 12/20 - \nTrain Loss: 2.0694 - Train Accuracy: 63.65% - \nVal Loss: 0.8643 - Val Accuracy: 79.94%\nEpoch  12\nBest model saved with accuracy: 81.13%\nEpoch 13/20 - \nTrain Loss: 2.0089 - Train Accuracy: 64.75% - \nVal Loss: 0.8136 - Val Accuracy: 81.13%\nEpoch  13\nEpoch 14/20 - \nTrain Loss: 2.0174 - Train Accuracy: 66.05% - \nVal Loss: 0.8206 - Val Accuracy: 81.10%\nEpoch  14\nEpoch 15/20 - \nTrain Loss: 2.0300 - Train Accuracy: 65.66% - \nVal Loss: 0.9131 - Val Accuracy: 80.79%\nEpoch  15\nBest model saved with accuracy: 81.43%\nEpoch 16/20 - \nTrain Loss: 1.9697 - Train Accuracy: 67.29% - \nVal Loss: 0.8461 - Val Accuracy: 81.43%\nEpoch  16\nEpoch 17/20 - \nTrain Loss: 1.9569 - Train Accuracy: 67.38% - \nVal Loss: 0.8788 - Val Accuracy: 80.80%\nEpoch  17\nEpoch 18/20 - \nTrain Loss: 1.9141 - Train Accuracy: 69.08% - \nVal Loss: 0.8220 - Val Accuracy: 81.00%\nEpoch  18\nBest model saved with accuracy: 81.45%\nEpoch 19/20 - \nTrain Loss: 1.8481 - Train Accuracy: 70.05% - \nVal Loss: 0.7798 - Val Accuracy: 81.45%\nEpoch  19\nBest model saved with accuracy: 81.88%\nEpoch 20/20 - \nTrain Loss: 1.8718 - Train Accuracy: 69.73% - \nVal Loss: 0.7749 - Val Accuracy: 81.88%\nTraining complete.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(Label(value='0.175 MB of 0.175 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "29f7b6735e504af88b58e0d31b54a2b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▄▅▆▆▆▇▇▇▇▇▇▇█▇█████</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆▇▇▇▇▇▇███████████</td></tr><tr><td>val_loss</td><td>█▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>train_accuracy</td><td>69.732</td></tr><tr><td>train_loss</td><td>1.8718</td></tr><tr><td>val_accuracy</td><td>81.88</td></tr><tr><td>val_loss</td><td>0.7749</td></tr></table><br/></div></div>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run <strong style=\"color:#cdcd00\">generous-sweep-1</strong> at: <a href='https://wandb.ai/gheorghitastefana-alexandru-ioan-cuza-university-iasi/training-cifar100/runs/cg9to7ic' target=\"_blank\">https://wandb.ai/gheorghitastefana-alexandru-ioan-cuza-university-iasi/training-cifar100/runs/cg9to7ic</a><br/> View project at: <a href='https://wandb.ai/gheorghitastefana-alexandru-ioan-cuza-university-iasi/training-cifar100' target=\"_blank\">https://wandb.ai/gheorghitastefana-alexandru-ioan-cuza-university-iasi/training-cifar100</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Find logs at: <code>./wandb/run-20241104_101214-cg9to7ic/logs</code>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5fu60cj0 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \talpha: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \taugmentation_scheme: randaugment\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: ./data\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdataset: CIFAR100\n\u001b[34m\u001b[1mwandb\u001b[0m: \teta_min: 1e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_delta: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_name: resnet18_resize\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.9\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnesterov: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_classes: 100\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 20\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_config: {'learning_rate': 0.001, 'optimizer': 'adamw'}\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpatience: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpatience_early_stopping: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpretrained: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler: cosineannealinglr\n\u001b[34m\u001b[1mwandb\u001b[0m: \tstop_mode: max\n\u001b[34m\u001b[1mwandb\u001b[0m: \tt_0: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tt_max: 100\n\u001b[34m\u001b[1mwandb\u001b[0m: \tt_mult: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_cutmix: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_mixup: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0005\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111366556666932, max=1.0)…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e98196c44904740afb16833110b0e33"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.18.3"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20241104_104627-5fu60cj0</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/gheorghitastefana-alexandru-ioan-cuza-university-iasi/training-cifar100/runs/5fu60cj0' target=\"_blank\">treasured-sweep-2</a></strong> to <a href='https://wandb.ai/gheorghitastefana-alexandru-ioan-cuza-university-iasi/training-cifar100' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/gheorghitastefana-alexandru-ioan-cuza-university-iasi/training-cifar100/sweeps/z9ns4r8w' target=\"_blank\">https://wandb.ai/gheorghitastefana-alexandru-ioan-cuza-university-iasi/training-cifar100/sweeps/z9ns4r8w</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/gheorghitastefana-alexandru-ioan-cuza-university-iasi/training-cifar100' target=\"_blank\">https://wandb.ai/gheorghitastefana-alexandru-ioan-cuza-university-iasi/training-cifar100</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View sweep at <a href='https://wandb.ai/gheorghitastefana-alexandru-ioan-cuza-university-iasi/training-cifar100/sweeps/z9ns4r8w' target=\"_blank\">https://wandb.ai/gheorghitastefana-alexandru-ioan-cuza-university-iasi/training-cifar100/sweeps/z9ns4r8w</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/gheorghitastefana-alexandru-ioan-cuza-university-iasi/training-cifar100/runs/5fu60cj0' target=\"_blank\">https://wandb.ai/gheorghitastefana-alexandru-ioan-cuza-university-iasi/training-cifar100/runs/5fu60cj0</a>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Epoch  0\nBest model saved with accuracy: 53.27%\nEpoch 1/20 - \nTrain Loss: 3.8850 - Train Accuracy: 18.01% - \nVal Loss: 1.8687 - Val Accuracy: 53.27%\nEpoch  1\nBest model saved with accuracy: 69.15%\nEpoch 2/20 - \nTrain Loss: 2.8968 - Train Accuracy: 41.68% - \nVal Loss: 1.2508 - Val Accuracy: 69.15%\nEpoch  2\nBest model saved with accuracy: 72.66%\nEpoch 3/20 - \nTrain Loss: 2.6161 - Train Accuracy: 49.04% - \nVal Loss: 1.0854 - Val Accuracy: 72.66%\nEpoch  3\nBest model saved with accuracy: 74.01%\nEpoch 4/20 - \nTrain Loss: 2.4926 - Train Accuracy: 52.30% - \nVal Loss: 1.0253 - Val Accuracy: 74.01%\nEpoch  4\nBest model saved with accuracy: 75.14%\nEpoch 5/20 - \nTrain Loss: 2.4600 - Train Accuracy: 53.72% - \nVal Loss: 1.0467 - Val Accuracy: 75.14%\nEpoch  5\nBest model saved with accuracy: 76.64%\nEpoch 6/20 - \nTrain Loss: 2.3495 - Train Accuracy: 56.48% - \nVal Loss: 0.9633 - Val Accuracy: 76.64%\nEpoch  6\nBest model saved with accuracy: 77.02%\nEpoch 7/20 - \nTrain Loss: 2.2972 - Train Accuracy: 57.99% - \nVal Loss: 0.9956 - Val Accuracy: 77.02%\nEpoch  7\nEpoch 8/20 - \nTrain Loss: 2.2241 - Train Accuracy: 60.39% - \nVal Loss: 0.9827 - Val Accuracy: 76.54%\nEpoch  8\nBest model saved with accuracy: 78.12%\nEpoch 9/20 - \nTrain Loss: 2.1682 - Train Accuracy: 61.34% - \nVal Loss: 0.8883 - Val Accuracy: 78.12%\nEpoch  9\nBest model saved with accuracy: 78.40%\nEpoch 10/20 - \nTrain Loss: 2.1277 - Train Accuracy: 62.73% - \nVal Loss: 0.8893 - Val Accuracy: 78.40%\nEpoch  10\nEpoch 11/20 - \nTrain Loss: 2.0516 - Train Accuracy: 64.03% - \nVal Loss: 0.9611 - Val Accuracy: 78.39%\nEpoch  11\nBest model saved with accuracy: 79.41%\nEpoch 12/20 - \nTrain Loss: 2.0408 - Train Accuracy: 65.57% - \nVal Loss: 0.8469 - Val Accuracy: 79.41%\nEpoch  12\nBest model saved with accuracy: 79.51%\nEpoch 13/20 - \nTrain Loss: 2.0343 - Train Accuracy: 65.26% - \nVal Loss: 0.8978 - Val Accuracy: 79.51%\nEpoch  13\nBest model saved with accuracy: 79.58%\nEpoch 14/20 - \nTrain Loss: 2.0138 - Train Accuracy: 65.94% - \nVal Loss: 0.8697 - Val Accuracy: 79.58%\nEpoch  14\nBest model saved with accuracy: 79.66%\nEpoch 15/20 - \nTrain Loss: 2.0313 - Train Accuracy: 66.20% - \nVal Loss: 0.8896 - Val Accuracy: 79.66%\nEpoch  15\nBest model saved with accuracy: 80.31%\nEpoch 16/20 - \nTrain Loss: 1.9337 - Train Accuracy: 67.88% - \nVal Loss: 0.8530 - Val Accuracy: 80.31%\nEpoch  16\nEpoch 17/20 - \nTrain Loss: 1.9491 - Train Accuracy: 67.98% - \nVal Loss: 0.8561 - Val Accuracy: 80.27%\nEpoch  17\nBest model saved with accuracy: 80.57%\nEpoch 18/20 - \nTrain Loss: 1.8935 - Train Accuracy: 70.05% - \nVal Loss: 0.8850 - Val Accuracy: 80.57%\nEpoch  18\nBest model saved with accuracy: 81.05%\nEpoch 19/20 - \nTrain Loss: 1.8814 - Train Accuracy: 69.66% - \nVal Loss: 0.8444 - Val Accuracy: 81.05%\nEpoch  19\nEpoch 20/20 - \nTrain Loss: 1.8934 - Train Accuracy: 69.24% - \nVal Loss: 0.8995 - Val Accuracy: 80.61%\nTraining complete.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(Label(value='0.176 MB of 0.176 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "36e35d4ba8f447f891937723e9aa2613"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▄▅▆▆▆▆▇▇▇▇▇▇▇▇█████</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▁▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆▆▇▇▇▇▇▇▇█████████</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>train_accuracy</td><td>69.236</td></tr><tr><td>train_loss</td><td>1.89344</td></tr><tr><td>val_accuracy</td><td>80.61</td></tr><tr><td>val_loss</td><td>0.89949</td></tr></table><br/></div></div>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run <strong style=\"color:#cdcd00\">treasured-sweep-2</strong> at: <a href='https://wandb.ai/gheorghitastefana-alexandru-ioan-cuza-university-iasi/training-cifar100/runs/5fu60cj0' target=\"_blank\">https://wandb.ai/gheorghitastefana-alexandru-ioan-cuza-university-iasi/training-cifar100/runs/5fu60cj0</a><br/> View project at: <a href='https://wandb.ai/gheorghitastefana-alexandru-ioan-cuza-university-iasi/training-cifar100' target=\"_blank\">https://wandb.ai/gheorghitastefana-alexandru-ioan-cuza-university-iasi/training-cifar100</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Find logs at: <code>./wandb/run-20241104_104627-5fu60cj0/logs</code>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}